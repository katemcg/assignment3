{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVFTG0FjBykl"
      },
      "source": [
        "<p align=\"center\"><h1 align=\"center\">QMSS5074GR <br> Projects in Advanced Machine Learning <br>Spring 2024 <br> FINAL PROJECT <br>Assignment 3 Part 2 [GROUP Assignment]</h1>\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "#### (Change Unis in the title of your notebook. The format should be `assignment3_Part2_GRPID_UNI1_UNI2... UNIn.ipynb`)\n",
        "#### **Your Unis** : Fill Here for all members (Comma separated).\n",
        "#### **Your Full names** : Fill Here (in same order as UNIs)\n",
        "#### **Your AI Model Share Usernames**: Fill Here (same order as UNIs)\n",
        "#### **Link to your Public Github repository** : Fill here (single link is expected)\n",
        "#### **Team member Contribution Percentages** : Fill here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXxGTgJz152A"
      },
      "source": [
        "## Stanford Sentiment Treebank - Movie Review Classification Competition\n",
        "\n",
        "\n",
        "## **Instructions: [Total 85 Points for this part]**\n",
        "1.   Get data in and set up X_train / X_test / y_train\n",
        "2.   Preprocess data using keras Tokenizer/ Write and Save Preprocessor function\n",
        "3. Fit model on preprocessed data and save preprocessor function and model\n",
        "4. Generate predictions from X_test data and submit model to competition\n",
        "5. Repeat submission process to improve place on leaderboard\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gSrVJwp3E9H"
      },
      "source": [
        "## 1. Get data in and set up X_train, X_test, y_train objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLTIaMB3ChSW"
      },
      "outputs": [],
      "source": [
        "#install aimodelshare library\n",
        "! pip install aimodelshare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3PiJXBhC5y-"
      },
      "outputs": [],
      "source": [
        "# Get competition data\n",
        "from aimodelshare import download_data\n",
        "download_data('public.ecr.aws/y2e2a1d6/sst2_competition_data-repository:latest')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTANT NOTE\n",
        "This X_test has no relation to test_sample.csv\n",
        "This X_test (downloaded from above link) is for prediction and submission to aimodelshare only. You can create a validation split from the X_train here or during instatiating the model.\n",
        "\n",
        "The test_sample.csv is meant for only inference from your best 3 saved models, and is only meant to be used in the final front-end notebook (See last section, Note that it has a very high weightage in this assignment). Predicting the labels in correct format on this test_sample may also require you to change your preprocessing and transform code. Ensure you code it correctly and that your final front-end notebook can predict on it appropriately.\n",
        "\n",
        "None of the training code in this notebook will go into your python files in Github or your front-end notebook (See last cell instructions)"
      ],
      "metadata": {
        "id": "Icj99lQhr__W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT0qFCZFNzHq"
      },
      "outputs": [],
      "source": [
        "# Set up X_train, X_test, and y_train_labels objects\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "\n",
        "X_train=pd.read_csv(\"sst2_competition_data/X_train.csv\")\n",
        "X_test=pd.read_csv(\"sst2_competition_data/X_test.csv\")\n",
        "y_train_labels=pd.read_csv(\"sst2_competition_data/y_train_labels.csv\")\n",
        "\n",
        "# Convert to series if your following preprocessor needs it\n",
        "\n",
        "\n",
        "# One hot encode encode Y data here\n",
        "\n",
        "\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEzPoXPj3V7u"
      },
      "source": [
        "##2.   Preprocess data using keras tokenizer / Write and Save Preprocessor function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16QV9Y9TC3B3"
      },
      "outputs": [],
      "source": [
        "# This preprocessor function makes use of the tf.keras tokenizer\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# [3 Pts]for this cell\n",
        "\n",
        "# Build vocabulary from training text data\n",
        "\n",
        "\n",
        "# preprocessor tokenizes words and makes sure all documents have the same length (perform padding with maxlen=40)\n",
        "\n",
        "\n",
        "print(preprocessor(X_train).shape)\n",
        "print(preprocessor(X_test).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X52kECL43b-O"
      },
      "source": [
        "##3. Fit model on preprocessed data and save preprocessor function and model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCbBf8j9ClYl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding,Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# A basic deep learning model (max 5 layers)  # [1 Pts]\n",
        "\n",
        "\n",
        "\n",
        "# Do validation split here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot history [1 Pts]"
      ],
      "metadata": {
        "id": "KAgqdiT1FBvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmJAnmO-5AcU"
      },
      "source": [
        "#### Save preprocessor function to local \"preprocessor.zip\" file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VGacc0LDaMA"
      },
      "outputs": [],
      "source": [
        "import aimodelshare as ai\n",
        "ai.export_preprocessor(preprocessor,\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOWBa8Cv5LdL"
      },
      "source": [
        "#### Save model to local \".onnx\" file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEhvnRiQDlY5"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHWkAzvX3m8O"
      },
      "source": [
        "## 4. Generate predictions from X_test data and submit model to competition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtgkM02MDpkO"
      },
      "outputs": [],
      "source": [
        "#Set credentials using modelshare.org username/password\n",
        "\n",
        "from aimodelshare.aws import set_credentials\n",
        "\n",
        "apiurl=\"https://rlxjxnoql9.execute-api.us-east-1.amazonaws.com/prod/m\" #This is the unique rest api that powers this specific Playground\n",
        "\n",
        "set_credentials(apiurl=apiurl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKNGSww8EGgi"
      },
      "outputs": [],
      "source": [
        "#Instantiate Competition\n",
        "\n",
        "mycompetition= ai.Competition(apiurl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ql4wksyEUnP"
      },
      "outputs": [],
      "source": [
        "#Submit Model 1:\n",
        "\n",
        "# [1 Pts]\n",
        "\n",
        "#-- Generate predicted y values on x_test (Model 1)\n",
        "\n",
        "\n",
        "# extract correct prediction labels\n",
        "\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN1zvAmNEq17"
      },
      "outputs": [],
      "source": [
        "# Get leaderboard to explore current best model architectures\n",
        "\n",
        "# Get raw data in pandas data frame\n",
        "data = mycompetition.get_leaderboard()\n",
        "\n",
        "# Stylize leaderboard data\n",
        "mycompetition.stylize_leaderboard(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwNKs0wP4r5s"
      },
      "source": [
        "## 5. Repeat submission process to improve place on leaderboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgSs5PAtPCZH"
      },
      "outputs": [],
      "source": [
        "# Train and submit model 2 using same preprocessor (note that you could save a new preprocessor, but we will use the same one for this example).\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten\n",
        "\n",
        "# Code an model with atleast 3 LSTM layers (with Embedding Layer) # [1 Pts]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot history [1 Pts]"
      ],
      "metadata": {
        "id": "8i_ivEC0FK8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does your model perform better or worse than your prior deep learning model? Why might that be the case?"
      ],
      "metadata": {
        "id": "jw7-1gh2FN_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer [1 Pts]"
      ],
      "metadata": {
        "id": "Y-3tc90OFM1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIdmSpYVPYAw"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model2, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model2.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nszPPrfwPlUk"
      },
      "outputs": [],
      "source": [
        "#Submit Model 2:\n",
        "\n",
        "#-- Generate predicted y values (Model 2)\n",
        "\n",
        "\n",
        "# extract correct prediction labels\n",
        "\n",
        "\n",
        "# Submit Model 2 to Competition Leaderboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLl7yLpVEx26"
      },
      "outputs": [],
      "source": [
        "# Compare two or more models\n",
        "data=mycompetition.compare_models([1, 2], verbose=1)\n",
        "mycompetition.stylize_compare(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl67bOO-lz9k"
      },
      "source": [
        "## Tune model within range of hyperparameters with Keras Tuner\n",
        "\n",
        "*Consult [documentation](https://keras.io/guides/keras_tuner/getting_started/) to see full functionality.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOGXwppct0KI"
      },
      "outputs": [],
      "source": [
        "! pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7o_zA3fv-ra"
      },
      "outputs": [],
      "source": [
        "#Separate validation data here\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train_split, x_val, y_train_split, y_val = train_test_split(\n",
        "     X_train, y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olzZ7aposfwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Define model structure & parameter search space with a function\n",
        "# code LSTM--based model with provision for searching hyperparameter space, return model  # [3 Pts]\n",
        "\n",
        "\n",
        "#initialize the keras search tuner (which will search through parameters)   # [3 Pts]\n",
        "# Define the following:\n",
        "#Your function that returns the model\n",
        "#objective to optimize\n",
        "#max number of trials to run during search\n",
        "#higher number reduces variance of results; guages model performance more accurately\n",
        "\n",
        "\n",
        "# Run the tuner feeding it the validation data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YaEbvJ2yND1"
      },
      "outputs": [],
      "source": [
        "# [3 Pts]\n",
        "\n",
        "# Build model with best hyperparameters\n",
        "\n",
        "# Get the top 2 hyperparameters.\n",
        "\n",
        "# Build the model with the best hp.\n",
        "\n",
        "# Fit with the entire dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What were the best hyperparameters found? Why might that be performing better than others?"
      ],
      "metadata": {
        "id": "51kUa-IJFmpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer [2 Pts]"
      ],
      "metadata": {
        "id": "fKrheWsOF0ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvLuIvOjHLH2"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(tuned_model, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"tuned_model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMidHABfHVN7"
      },
      "outputs": [],
      "source": [
        "#Submit Model 3:\n",
        "\n",
        "#-- Generate predicted y values (Model 3)\n",
        "\n",
        "\n",
        "# extract correct prediction labels\n",
        "\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn-Tp-zpJVGT"
      },
      "outputs": [],
      "source": [
        "# Get leaderboard\n",
        "\n",
        "data = mycompetition.get_leaderboard()\n",
        "mycompetition.stylize_leaderboard(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKvPKyb7hT0h"
      },
      "outputs": [],
      "source": [
        "# Compare two or more models\n",
        "data=mycompetition.compare_models([1, 2, 3], verbose=1)\n",
        "mycompetition.stylize_compare(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 more models [30 Pts]"
      ],
      "metadata": {
        "id": "-nU8VR9uT3OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train three more prediction models to try to predict the SST sentiment\n",
        "dataset well.\n",
        "\n",
        "○ Use Conv1d layers in first model [ 2 Pts]\n",
        "\n",
        "○ Use Transfer learning with Glove Embeddings for 2nd model [10 Pts]\n",
        "\n",
        "○ Third model can be any Transfer learning model of your choice (Transformer architecture required, eg. BERT and related) [10 Pts]"
      ],
      "metadata": {
        "id": "eefOmAIaJYnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1"
      ],
      "metadata": {
        "id": "xNzUJrsxJlU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2"
      ],
      "metadata": {
        "id": "kYqWJ8VdJ_kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3"
      ],
      "metadata": {
        "id": "SmWC5tG0J_nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra credits : Train any models of your choice [+10 Extra Points]"
      ],
      "metadata": {
        "id": "PKjNrHW2fZOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Any modern models of your choice (should be released after BERT)"
      ],
      "metadata": {
        "id": "Mjfe4bPZflxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tabularize results from ALL your models in this notebook and compare performance [5 Pts]"
      ],
      "metadata": {
        "id": "E15c5GrbKT_D"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwB8haTTK6Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results Discussion : Point out why specific models may have performed better than others. and discuss failures if any. [3 Pts]"
      ],
      "metadata": {
        "id": "-NH7R7sNLGLK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FLbR4oEmLJJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Inference pipeline on unseen dataset [45 Points (35 Pts + 10 Extra Credits)]"
      ],
      "metadata": {
        "id": "UBwhx3wfUIsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Github Repo: </b>\n",
        "\n",
        "## **Part 1. [25 Pts]**\n",
        "\n",
        "You will exercise **Modularization** and Good Github organization: You should take the relevant code and make it into classes across different python (.py) files so that you can just import those files and/or classes and/or functions and/or modules, instantiate the model using them, load weights from your saved models, and start predicting on any new set of test reviews (an unseen larger test dataset).\n",
        "\n",
        "Essentially, you should have a simple and clean **\"front-end\" notebook** where you download your trained and saved models from say a personal drive link, and have an inference pipeline ready within about **10 lines of code** to predict on any new unseen Movie reviews dataset after loading it from a csv.\n",
        "\n",
        "This time you will provide a choice of 3 trained models to choose from (atleast 2 from the last 3 models section + extra credit section if you did it) . You will give us a list of strings as a comment, and should provide the functionality to just specify the model to load using one of the 3 strings. An outline is detailed below.\n",
        "\n",
        "<br><br>\n",
        "**We have provided a test_sample.csv with 100 rows for you to test your front-end notebook.**\n",
        "\n",
        "Your notebook should import relevant classes and modules on the back-end, and should load weights and subsequently just take the \"review\" column of given test_sample.csv and predict 'positive' or 'negative' sentiment on it (see outline of code below). Use the \"sentiment\" column from the test_sample csv ONLY when finally comparing your own predictions with it and output a classification report.\n",
        "\n",
        "You should use a local environment with all packages installed and an IDE like VScode or PyCharm etc. to be able to do this section efficiently. There is no aimodelshare invlved in this section, and there shouldn't be any mention of it.\n",
        "\n",
        "\n",
        "\n",
        "<br><br>\n",
        "### Environment Requirements\n",
        "Include a pip requirements.txt file for your environment.\n",
        "Use the following command to generate it automatically:\n",
        "\n",
        "```$ pip freeze > requirements.txt```\n",
        "\n",
        "When evaluating this section, we will run\n",
        "\n",
        "```pip install -r requirements.txt```\n",
        "\n",
        "to recreate the exact environment you have locally and then test your code.\n",
        "\n",
        "**ENSURE** that this works as intended by creating another new virtual conda environment, cloning your own latest Github commit locally from scratch, installing packages using your requirements.txt file, and following all the instructions you provide in the Readme and the front-end notebook to run the inference pipeline on the sample data. We will do the exact same steps to replicate it, and then substitute test sample with the actual dataset to evaluate this section.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nHZKXkVAPUbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline for front-end notebook:"
      ],
      "metadata": {
        "id": "1DfTz2lgAIqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## You will include code to load the test_sample.csv in your Front-end notebok\n",
        "## Note that this csv can be of any large length, and you will predict on it using one of 3 models you provide weights for."
      ],
      "metadata": {
        "id": "fGSNFMqRPvWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide a comment like this:\n",
        "\n",
        "\n",
        "\n",
        "### Hi User! Please choose a model between [\"Glove150d\", \"DistillBERT\", \"GPT2\"]\n",
        "\n",
        "model_name = \"DistillBERT\"  # Choice to modify model name from your given list\n",
        "\n",
        "\n",
        "## Now call appropriate class/function from backend to download relevant weights, instantiate specified model and load the weights, and return the loaded model, ready to predict\n",
        "\n",
        "## load the test csv:\n",
        "import pandas as pd\n",
        "\n",
        "test_data = pd.read_csv(\"test_sample.csv\")\n",
        "test_reviews=test_data.review\n"
      ],
      "metadata": {
        "id": "bGTkJpjWP4VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Send the test reviews to a backend function after preprocessing to predict sentiment : USE \"tqdm\" while predicting to show progress bar, transform predictions as needed\n",
        "\n",
        "## preprocessing and any other intermediate steps should be through backend code, not in the front-end notebook\n",
        "\n",
        "## pull out test_data.sentiment now (ground truth), transform as needed and compare your predictions with it\n",
        "\n",
        "## Output the classification report on these test reviews and plot confusion matrix at the end of the front-end notebook (You will actually be tested on a much larger test dataset, but it would be of the same format)"
      ],
      "metadata": {
        "id": "HdmOat5TQPPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make sure your notebook can run your inference pipeline on the loaded test_sample.csv reviews.  \n",
        "\n",
        "# **You will recieve a ZERO (0) for this section if any of the following cases are true:**\n",
        "### - If there is no front-end notebook, or no python (.py) files where you have declared classes, modules and functions to delegate all the work in backend\n",
        "### - If there is no requirements.txt file in your repo\n",
        "### - If after installing requirements in a new environment, following the  instructions in your Readme or Notebook does not lead to any valid output upon running the front-end notebook.\n",
        "### - If your notebook does not output the final classification report and matrix for at least 1 model after predicting on test_sample\n",
        "### - If you include or run any Training code in your front-end notebook or python files in github repository\n",
        "### - If you include more than 30 lines of code in the front-end notebook (apart from importing your functions, classes from your modules)\n",
        "### - If you attempt to use the 'sentiment' column from the test_sample for any  purpose other than comparison of your final predictions with it, and transforming it if needed.\n",
        "\n",
        "\n",
        "\n",
        "<br><br>\n",
        "# **You will recieve 25 + 10 EXTRA points if you do this section well.**\n",
        "\n",
        "### How to do this section well:\n",
        "\n",
        "Get a proper understanding of object-oriented programming in python. Learn what classes and modules are if you don't know already. Try to **encapsulate all code** within appropriately named classes and its functions. Each class and function should have a specific isloated purpose. All the classes and functions you design would talk to each other efficiently and handle everything in the background without needing to get back to the front-end notebook for outputting or retreiving intermediate objects.\n",
        "\n",
        "In the front-end notebook, you essentially only need to include code to send the model name to backend using 1 function, get the loaded model back, read and send the test reviews to backend for predicting with this loaded model, then load the ground truth and straight up output the classification report from the backend (less than 10 lines of code)\n",
        "\n",
        "The majority of your code should never be exposed directly in python files without being inside a class or a function. Only explicitly needed global variables, imports or declarations should be done outside the scope of a function or a class.\n",
        "\n",
        "<br><br>\n",
        "### Documentation\n",
        "\n",
        "Include **Docstrings** for each function and class, define the input parameters and outputs, what they are and what data type they are expected to be in.\n",
        "\n",
        "Example of a good docstring:\n",
        "```python\n",
        "def calculate_rectangle_area(length: float, width: float) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the area of a rectangle.\n",
        "\n",
        "    This function takes the length and width of a rectangle and returns its area. The calculation assumes that the input values are in the same unit and will return the area in that unit squared.\n",
        "\n",
        "    Parameters:\n",
        "    - length (float): The length of the rectangle. Must be a positive number.\n",
        "    - width (float): The width of the rectangle. Must be a positive number.\n",
        "\n",
        "    Returns:\n",
        "    - area (float): The calculated area of the rectangle in the unit squared of the input measurements.\n",
        "\n",
        "    Raises:\n",
        "    - ValueError: If either the length or width is non-positive.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if length <= 0 or width <= 0:\n",
        "        raise ValueError(\"Length and width must be positive numbers.\")\n",
        "\n",
        "    area = length * width\n",
        "\n",
        "    return area\n",
        "```"
      ],
      "metadata": {
        "id": "7DMPWpf6bY6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<br><br>\n",
        "### **Part 2. [5 Pts]**\n",
        "\n",
        "In the **README.md** file of your repository should be an overall outline of your experiments, including the models you trained succesfully and have results for - summarized in a tabular way. You can include other details in your readme as well, but not too detailed. Consider this an exposition of your project for someone who does not know anything about this project.\n",
        "\n",
        "\n",
        "Include instructions on how to install requirements, how to run your front-end notebook and describe which 3 models you gave choice to load and predict on.\n",
        "\n",
        "\n",
        "Include a tree structure of your files in the github repository.\n",
        "\n",
        "Example:\n",
        "```\n",
        "$ tree\n",
        ".\n",
        "├── dir1\n",
        "│   ├── file11.ext\n",
        "│   └── file12.ext\n",
        "├── dir2\n",
        "│   ├── file21.ext\n",
        "│   ├── file22.ext\n",
        "│   └── file23.ext\n",
        "├── dir3\n",
        "├── file_in_root.ext\n",
        "└── README.md\n",
        "\n",
        "3 directories, 7 files\n",
        "```\n",
        "\n",
        "Look at different readme's across github to see how they are organized. Refer to [this resource](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to learn how to format the file.\n",
        "\n",
        "- Example of a [Good Readme File](https://github.com/hankcs/HanLP) (We have no affiliations with Author)\n",
        "- Example of a [Bad Readme File](https://github.com/tstran155/AG-News-Topic-Classification-and-Topic-Modeling-using-Natural-Language-Processing)\n",
        "- Example of an [Acceptable/OK Readme File](https://github.com/lonePatient/Bert-Multi-Label-Text-Classification)\n",
        ": This is the minimum level at which you should have your Readme\n",
        "\n",
        "Note that these examples are different from the last assignment.\n",
        "\n",
        "<br><br>\n",
        "### **Part 3. [5 Pts]**\n",
        "\n",
        "In the **Final report** (.ipynb) notebook, put whatever code, visuals and results you think are relevant. It is open ended, you should not just post a copy of this notebook for it. It should be brief, clean and succinct as has been instructed before. No residues from the assignment should be present, it should look like a professional report of experiments conducted.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tjopqFqUs0Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Do not forget to link the final Github repository containing the Front-end running notebook, your code, Readme and Final report at the top of this notebook."
      ],
      "metadata": {
        "id": "H4dPFDuKU-zN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GPcFanPsU_Qf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}