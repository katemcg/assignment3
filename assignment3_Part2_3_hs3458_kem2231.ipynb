{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVFTG0FjBykl"
      },
      "source": [
        "<p align=\"center\"><h1 align=\"center\">QMSS5074GR <br> Projects in Advanced Machine Learning <br>Spring 2024 <br> FINAL PROJECT <br>Assignment 3 Part 2 [GROUP Assignment]</h1>\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "#### (Change Unis in the title of your notebook. The format should be `assignment3_Part2_GRPID_UNI1_UNI2... UNIn.ipynb`)\n",
        "#### **Your Unis** : Fill Here for all members (Comma separated).\n",
        "#### **Your Full names** : Fill Here (in same order as UNIs)\n",
        "#### **Your AI Model Share Usernames**: Fill Here (same order as UNIs)\n",
        "#### **Link to your Public Github repository** : Fill here (single link is expected)\n",
        "#### **Team member Contribution Percentages** : Fill here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest achievable marks are 120, with there being 20 extra credits. The overall distribution of marks is as follows:\n",
        "\n",
        "Part 1:\n",
        "EDA and basic ML models : [15 points]\n",
        "\n",
        "Part 2:\n",
        "Deep Learning Section: [60 Total possible]\n",
        "Recurrent models : 20 points\n",
        "3 other models (CNN and Transformers) : 22 points + 10 Extra credits\n",
        "Results : 8 points\n",
        "\n",
        "\n",
        "Github Section : [45 Total possible]\n",
        "Inference pipeline : 25 Points + 10 Extra credits\n",
        "Readme + Report : 10 points\n",
        "\n",
        "Heavy weightage (45 total possible marks) is assigned to the last section. Please be mindful of the time required to complete the last section as it is non-trivial and the most important part of the final project. Read instructions on how to score Zero marks and how to score 45 marks on this."
      ],
      "metadata": {
        "id": "34O15Pnn4qTr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXxGTgJz152A"
      },
      "source": [
        "## Stanford Sentiment Treebank - Movie Review Classification Competition\n",
        "\n",
        "\n",
        "## **Instructions: [Total 85 Points for this part]**\n",
        "1.   Get data in and set up X_train / X_test / y_train\n",
        "2.   Preprocess data using keras Tokenizer/ Write and Save Preprocessor function\n",
        "3. Fit model on preprocessed data and save preprocessor function and model\n",
        "4. Generate predictions from X_test data and submit model to competition\n",
        "5. Repeat submission process to improve place on leaderboard\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gSrVJwp3E9H"
      },
      "source": [
        "## 1. Get data in and set up X_train, X_test, y_train objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLTIaMB3ChSW",
        "outputId": "f01115cc-5fc9-4bae-ddb1-3697b0fec22c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aimodelshare\n",
            "  Downloading aimodelshare-0.1.11-py3-none-any.whl (975 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.8/975.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3==1.26.69 (from aimodelshare)\n",
            "  Downloading boto3-1.26.69-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore==1.29.82 (from aimodelshare)\n",
            "  Downloading botocore-1.29.82-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (from aimodelshare) (1.2.2)\n",
            "Collecting onnx==1.13.1 (from aimodelshare)\n",
            "  Downloading onnx-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxconverter-common>=1.7.0 (from aimodelshare)\n",
            "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from aimodelshare) (2023.12.25)\n",
            "Collecting keras2onnx>=1.7.0 (from aimodelshare)\n",
            "  Downloading keras2onnx-1.7.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.3/96.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.12 in /usr/local/lib/python3.10/dist-packages (from aimodelshare) (2.15.0)\n",
            "Collecting tf2onnx (from aimodelshare)\n",
            "  Downloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting skl2onnx>=1.14.0 (from aimodelshare)\n",
            "  Downloading skl2onnx-1.16.0-py2.py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.5/298.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.7.0 (from aimodelshare)\n",
            "  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from aimodelshare) (2.2.1+cu121)\n",
            "Collecting pydot==1.3.0 (from aimodelshare)\n",
            "  Downloading pydot-1.3.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting importlib-resources==5.10.0 (from aimodelshare)\n",
            "  Downloading importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
            "Collecting onnxmltools>=1.6.1 (from aimodelshare)\n",
            "  Downloading onnxmltools-1.12.0-py2.py3-none-any.whl (329 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pympler==0.9 (from aimodelshare)\n",
            "  Downloading Pympler-0.9.tar.gz (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.4/178.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docker==5.0.0 (from aimodelshare)\n",
            "  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wget==3.2 (from aimodelshare)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyJWT>=2.4.0 (from aimodelshare)\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: seaborn>=0.11.2 in /usr/local/lib/python3.10/dist-packages (from aimodelshare) (0.13.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.10/dist-packages (from aimodelshare) (1.6.3)\n",
            "Collecting shortuuid>=1.0.8 (from aimodelshare)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.10/dist-packages (from aimodelshare) (5.9.5)\n",
            "Requirement already satisfied: pathlib>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from aimodelshare) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.10/dist-packages (from aimodelshare) (3.20.3)\n",
            "Collecting dill (from aimodelshare)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikeras (from aimodelshare)\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse==1.6.3->aimodelshare) (0.43.0)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse==1.6.3->aimodelshare) (1.16.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3==1.26.69->aimodelshare)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3==1.26.69->aimodelshare)\n",
            "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.29.82->aimodelshare) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4 (from botocore==1.29.82->aimodelshare)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker==5.0.0->aimodelshare) (1.7.0)\n",
            "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in /usr/local/lib/python3.10/dist-packages (from docker==5.0.0->aimodelshare) (2.31.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from onnx==1.13.1->aimodelshare) (1.25.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx==1.13.1->aimodelshare) (4.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot==1.3.0->aimodelshare) (3.1.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->aimodelshare) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->aimodelshare) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->aimodelshare) (3.4.0)\n",
            "Collecting fire (from keras2onnx>=1.7.0->aimodelshare)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxconverter-common>=1.7.0->aimodelshare) (24.0)\n",
            "Collecting protobuf>=3.20.1 (from aimodelshare)\n",
            "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.7.0->aimodelshare)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->aimodelshare) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->aimodelshare) (1.12)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn>=0.11.2->aimodelshare) (2.0.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn>=0.11.2->aimodelshare) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (1.4.0)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (3.3.0)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow>=2.12 (from aimodelshare)\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=3.10.0 (from tensorflow>=2.12->aimodelshare)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes~=0.3.1 (from tensorflow>=2.12->aimodelshare)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow>=2.12 (from aimodelshare)\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.7/479.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1 (from tensorflow>=2.12->aimodelshare)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (1.62.2)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow>=2.12->aimodelshare)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.16.6 (from onnx==1.13.1->aimodelshare)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow>=2.12 (from aimodelshare)\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading tensorflow-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (0.4.26)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow>=2.12->aimodelshare)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow>=2.12 (from aimodelshare)\n",
            "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.16.6 (from onnx==1.13.1->aimodelshare)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting skl2onnx>=1.14.0 (from aimodelshare)\n",
            "  Downloading skl2onnx-1.15.0-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.7/294.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting onnxruntime>=1.7.0 (from aimodelshare)\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.14.1-cp310-cp310-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.14.0-cp310-cp310-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.13.1-cp310-cp310-manylinux_2_27_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.12.1-cp310-cp310-manylinux_2_27_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading onnxruntime-1.12.0-cp310-cp310-manylinux_2_27_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxmltools>=1.6.1 (from aimodelshare)\n",
            "  Downloading onnxmltools-1.11.2-py2.py3-none-any.whl (322 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxconverter-common>=1.7.0 (from aimodelshare)\n",
            "  Downloading onnxconverter_common-1.13.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (0.36.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.12->aimodelshare) (2.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->aimodelshare) (3.13.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->aimodelshare) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->aimodelshare) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->aimodelshare) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->aimodelshare) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.1->aimodelshare)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "INFO: pip is looking at multiple versions of scikeras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scikeras (from aimodelshare)\n",
            "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11.2->aimodelshare) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11.2->aimodelshare) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11.2->aimodelshare) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11.2->aimodelshare) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11.2->aimodelshare) (9.4.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn>=0.11.2->aimodelshare) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn>=0.11.2->aimodelshare) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare) (2024.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.12->aimodelshare) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.12->aimodelshare) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.12->aimodelshare) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.12->aimodelshare) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.12->aimodelshare) (3.0.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.7.0->aimodelshare)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->aimodelshare) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.7.0->aimodelshare) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.12->aimodelshare) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.12->aimodelshare) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.12->aimodelshare) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.12->aimodelshare) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.12->aimodelshare) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.12->aimodelshare) (3.2.2)\n",
            "Building wheels for collected packages: Pympler, wget, fire\n",
            "  Building wheel for Pympler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pympler: filename=Pympler-0.9-py3-none-any.whl size=164823 sha256=8ab68242d21634c30235cd594ad1e815c460d70e3f3d547bed8e2bdbddb5d330\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/8b/87/a090abb9a44f310f0126252e5e291d4757c8f0520cf66e1eff\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=1761f61c77165d81c26214a0adfb410c5bcb16fc8a19294e8a08c2fc6c4c5c12\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=d0370b7abf897ea8ecc8958ff5d3151779d7ba323a9711c1f73034e00c47c277\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built Pympler wget fire\n",
            "Installing collected packages: wget, Pympler, urllib3, shortuuid, PyJWT, pydot, onnx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, importlib-resources, humanfriendly, fire, dill, onnxmltools, onnxconverter-common, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, botocore, tf2onnx, skl2onnx, scikeras, s3transfer, onnxruntime, nvidia-cusolver-cu12, keras2onnx, docker, boto3, aimodelshare\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: PyJWT\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 1.4.2\n",
            "    Uninstalling pydot-1.4.2:\n",
            "      Successfully uninstalled pydot-1.4.2\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib_resources 6.4.0\n",
            "    Uninstalling importlib_resources-6.4.0:\n",
            "      Successfully uninstalled importlib_resources-6.4.0\n",
            "Successfully installed PyJWT-2.8.0 Pympler-0.9 aimodelshare-0.1.11 boto3-1.26.69 botocore-1.29.82 coloredlogs-15.0.1 dill-0.3.8 docker-5.0.0 fire-0.6.0 humanfriendly-10.0 importlib-resources-5.10.0 jmespath-1.0.1 keras2onnx-1.7.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 onnx-1.13.1 onnxconverter-common-1.13.0 onnxmltools-1.12.0 onnxruntime-1.17.3 pydot-1.3.0 s3transfer-0.6.2 scikeras-0.12.0 shortuuid-1.0.13 skl2onnx-1.16.0 tf2onnx-1.16.1 urllib3-1.26.18 wget-3.2\n"
          ]
        }
      ],
      "source": [
        "#install aimodelshare library\n",
        "! pip install aimodelshare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3PiJXBhC5y-",
        "outputId": "b3923be2-bb67-4dc8-aeda-05ad6faf915f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Please install pyspark to enable pyspark features\n",
            "Downloading [=============================================>   ]\n",
            "\n",
            "Data downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Get competition data\n",
        "from aimodelshare import download_data\n",
        "download_data('public.ecr.aws/y2e2a1d6/sst2_competition_data-repository:latest')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTANT NOTE\n",
        "This X_test has no relation to test_sample.csv\n",
        "This X_test (downloaded from above link) is for prediction and submission to aimodelshare only. You can create a validation split from the X_train here or during instatiating the model.\n",
        "\n",
        "The test_sample.csv is meant for only inference from your best 3 saved models, and is only meant to be used in the final front-end notebook (See last section, Note that it has a very high weightage in this assignment). Predicting the labels in correct format on this test_sample may also require you to change your preprocessing and transform code. Ensure you code it correctly and that your final front-end notebook can predict on it appropriately.\n",
        "\n",
        "None of the training code in this notebook will go into your python files in Github or your front-end notebook (See last cell instructions)"
      ],
      "metadata": {
        "id": "Icj99lQhr__W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT0qFCZFNzHq",
        "outputId": "a8cf9d46-6237-4fd0-a0d4-0c6e928688ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text\n",
              "0  The Rock is destined to be the 21st Century 's...\n",
              "1  The gorgeously elaborate continuation of `` Th...\n",
              "2  Singer/composer Bryan Adams contributes a slew...\n",
              "3               Yet the act is still charming here .\n",
              "4  Whether or not you 're enlightened by any of D..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-445fc531-fe59-4165-a11f-6fd4d8f8a84b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Yet the act is still charming here .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Whether or not you 're enlightened by any of D...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-445fc531-fe59-4165-a11f-6fd4d8f8a84b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-445fc531-fe59-4165-a11f-6fd4d8f8a84b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-445fc531-fe59-4165-a11f-6fd4d8f8a84b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e1a776ca-83bc-47a8-86ce-86d11e6f775c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e1a776ca-83bc-47a8-86ce-86d11e6f775c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e1a776ca-83bc-47a8-86ce-86d11e6f775c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train",
              "summary": "{\n  \"name\": \"X_train\",\n  \"rows\": 6920,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6911,\n        \"samples\": [\n          \"Snipes is both a snore and utter tripe .\",\n          \"A wild ride juiced with enough energy and excitement for at least three films .\",\n          \"Its screenplay serves as auto-critique , and its clumsiness as its own most damning censure .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Set up X_train, X_test, and y_train_labels objects\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "\n",
        "X_train=pd.read_csv(\"sst2_competition_data/X_train.csv\")\n",
        "X_test=pd.read_csv(\"sst2_competition_data/X_test.csv\")\n",
        "y_train_labels=pd.read_csv(\"sst2_competition_data/y_train_labels.csv\")\n",
        "\n",
        "# Convert to series if your following preprocessor needs it\n",
        "\n",
        "\n",
        "# One hot encode encode Y data here\n",
        "y_train = pd.get_dummies(y_train_labels)\n",
        "\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEzPoXPj3V7u"
      },
      "source": [
        "##2.   Preprocess data using keras tokenizer / Write and Save Preprocessor function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16QV9Y9TC3B3",
        "outputId": "1a6057e2-b69b-4517-ee0a-d9d816bd35c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded Sequences:\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  2 3 4 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5\n",
            "  6 7 8 1]]\n",
            "Tokenizer Word Index:\n",
            "{'you': 1, 'hello': 2, 'how': 3, 'are': 4, \"i'm\": 5, 'doing': 6, 'fine': 7, 'thank': 8}\n",
            "(1, 40)\n",
            "(1, 40)\n"
          ]
        }
      ],
      "source": [
        "# This preprocessor function makes use of the tf.keras tokenizer\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# [3 Pts]for this cell\n",
        "\n",
        "# Build vocabulary from training text data\n",
        "\n",
        "\n",
        "# preprocessor tokenizes words and makes sure all documents have the same length (perform padding with maxlen=40)\n",
        "\n",
        "\n",
        "def preprocessor(texts):\n",
        "    \"\"\"\n",
        "    Preprocesses a list of texts using Tokenizer and pads sequences to a fixed length.\n",
        "\n",
        "    Args:\n",
        "    texts: List of strings representing input texts.\n",
        "    maxlen: Maximum length of sequences after padding.\n",
        "\n",
        "    Returns:\n",
        "    padded_sequences: Numpy array of padded sequences.\n",
        "    tokenizer: Tokenizer object fitted on the input texts.\n",
        "    \"\"\"\n",
        "    # Initialize Tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "\n",
        "    # Fit tokenizer on input texts\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    # Convert texts to sequences of integers\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Pad sequences to a fixed length\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=40)\n",
        "\n",
        "    return padded_sequences, tokenizer\n",
        "\n",
        "# Example usage:\n",
        "texts = [\"Hello, how are you?\", \"I'm doing fine, thank you!\"]\n",
        "padded_sequences, tokenizer = preprocessor(texts)\n",
        "print(\"Padded Sequences:\")\n",
        "print(padded_sequences)\n",
        "print(\"Tokenizer Word Index:\")\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "print(preprocessor(X_train)[0].shape)\n",
        "print(preprocessor(X_test)[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X52kECL43b-O"
      },
      "source": [
        "##3. Fit model on preprocessed data and save preprocessor function and model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCbBf8j9ClYl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding,Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# A basic deep learning model (max 5 layers)  # [1 Pts]\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=40))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "checkpoint_path = \"model_checkpoint.h5\"\n",
        "\n",
        "red_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.1) # dividing lr by 10 when val_accuracy fails to improve after 3 epochs\n",
        "mod_check = ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Fit the model with validation split\n",
        "history = model.fit(preprocessor(X_train)[0], y_train, batch_size=32, epochs=30, validation_split=0.2, callbacks = [red_lr, mod_check, early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot history [1 Pts]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KAgqdiT1FBvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmJAnmO-5AcU"
      },
      "source": [
        "#### Save preprocessor function to local \"preprocessor.zip\" file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VGacc0LDaMA"
      },
      "outputs": [],
      "source": [
        "import aimodelshare as ai\n",
        "ai.export_preprocessor(preprocessor,\"preprocessor.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOWBa8Cv5LdL"
      },
      "source": [
        "#### Save model to local \".onnx\" file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEhvnRiQDlY5"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHWkAzvX3m8O"
      },
      "source": [
        "## 4. Generate predictions from X_test data and submit model to competition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtgkM02MDpkO"
      },
      "outputs": [],
      "source": [
        "#Set credentials using modelshare.org username/password\n",
        "\n",
        "from aimodelshare.aws import set_credentials\n",
        "\n",
        "apiurl=\"https://rlxjxnoql9.execute-api.us-east-1.amazonaws.com/prod/m\" #This is the unique rest api that powers this specific Playground\n",
        "\n",
        "set_credentials(apiurl=apiurl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKNGSww8EGgi"
      },
      "outputs": [],
      "source": [
        "#Instantiate Competition\n",
        "\n",
        "mycompetition= ai.Competition(apiurl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ql4wksyEUnP"
      },
      "outputs": [],
      "source": [
        "#Submit Model 1:\n",
        "\n",
        "# [1 Pts]\n",
        "\n",
        "#-- Generate predicted y values on x_test (Model 1)\n",
        "\n",
        "\n",
        "# extract correct prediction labels\n",
        "\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN1zvAmNEq17"
      },
      "outputs": [],
      "source": [
        "# Get leaderboard to explore current best model architectures\n",
        "\n",
        "# Get raw data in pandas data frame\n",
        "data = mycompetition.get_leaderboard()\n",
        "\n",
        "# Stylize leaderboard data\n",
        "mycompetition.stylize_leaderboard(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwNKs0wP4r5s"
      },
      "source": [
        "## 5. Repeat submission process to improve place on leaderboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgSs5PAtPCZH"
      },
      "outputs": [],
      "source": [
        "# Train and submit model 2 using same preprocessor (note that you could save a new preprocessor, but we will use the same one for this example).\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten\n",
        "\n",
        "# Code an model with atleast 3 LSTM layers (with Embedding Layer) # [1 Pts]\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=40))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model with validation split\n",
        "history = model.fit(preprocessor(X_train)[0], y_train, batch_size=32, epochs=30, validation_split=0.2, callbacks = [red_lr, mod_check, early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot history [1 Pts]\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8i_ivEC0FK8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does your model perform better or worse than your prior deep learning model? Why might that be the case?"
      ],
      "metadata": {
        "id": "jw7-1gh2FN_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer [1 Pts]"
      ],
      "metadata": {
        "id": "Y-3tc90OFM1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIdmSpYVPYAw"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(model2, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model2.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nszPPrfwPlUk"
      },
      "outputs": [],
      "source": [
        "#Submit Model 2:\n",
        "\n",
        "#-- Generate predicted y values (Model 2)\n",
        "\n",
        "\n",
        "# extract correct prediction labels\n",
        "\n",
        "\n",
        "# Submit Model 2 to Competition Leaderboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLl7yLpVEx26"
      },
      "outputs": [],
      "source": [
        "# Compare two or more models\n",
        "data=mycompetition.compare_models([1, 2], verbose=1)\n",
        "mycompetition.stylize_compare(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl67bOO-lz9k"
      },
      "source": [
        "## Tune model within range of hyperparameters with Keras Tuner\n",
        "\n",
        "\n",
        "*Consult [documentation](https://keras.io/guides/keras_tuner/getting_started/) to see full functionality.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOGXwppct0KI"
      },
      "outputs": [],
      "source": [
        "! pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7o_zA3fv-ra"
      },
      "outputs": [],
      "source": [
        "#Separate validation data here\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train_split, x_val, y_train_split, y_val = train_test_split(\n",
        "     X_train, y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olzZ7aposfwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, Dropout\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Define model structure & parameter search space with a function\n",
        "# code LSTM--based model with provision for searching hyperparameter space, return model  # [3 Pts]\n",
        "# Define the model structure with hyperparameters\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=40))\n",
        "    for i in range(hp.Int('num_lstm_layers', 1, 3)):  # Number of LSTM layers (1 to 3)\n",
        "        model.add(LSTM(units=hp.Int(f'lstm_{i}_units', min_value=32, max_value=128, step=32), return_sequences=True))\n",
        "        model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)))  # Dropout after each LSTM layer\n",
        "    model.add(LSTM(16))  # Final LSTM layer\n",
        "    model.add(Dropout(0.2))  # Dropout after the final LSTM layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "#initialize the keras search tuner (which will search through parameters)   # [3 Pts]\n",
        "# Define the following:\n",
        "#Your function that returns the model\n",
        "#objective to optimize\n",
        "#max number of trials to run during search\n",
        "#higher number reduces variance of results; guages model performance more accurately\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10\n",
        ")\n",
        "\n",
        "# Run the tuner feeding it the validation data\n",
        "tuner.search(x_val, y_val, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YaEbvJ2yND1"
      },
      "outputs": [],
      "source": [
        "# [3 Pts]\n",
        "\n",
        "# Build model with best hyperparameters\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=2)  # Get the top 2 hyperparameters\n",
        "\n",
        "# Iterate over the top hyperparameters\n",
        "for hp in best_hps:\n",
        "    # Build the model with the best hyperparameters\n",
        "    model = build_model(hp)\n",
        "    # Fit the model with the entire dataset\n",
        "    model.fit(preprocessor(X_train)[0], y_train, epochs=30, batch_size=32, callbacks = [red_lr, mod_check, early_stop])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What were the best hyperparameters found? Why might that be performing better than others?"
      ],
      "metadata": {
        "id": "51kUa-IJFmpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer [2 Pts]"
      ],
      "metadata": {
        "id": "fKrheWsOF0ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvLuIvOjHLH2"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(tuned_model, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"tuned_model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMidHABfHVN7"
      },
      "outputs": [],
      "source": [
        "#Submit Model 3:\n",
        "\n",
        "#-- Generate predicted y values (Model 3)\n",
        "\n",
        "\n",
        "# extract correct prediction labels\n",
        "\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn-Tp-zpJVGT"
      },
      "outputs": [],
      "source": [
        "# Get leaderboard\n",
        "\n",
        "data = mycompetition.get_leaderboard()\n",
        "mycompetition.stylize_leaderboard(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKvPKyb7hT0h"
      },
      "outputs": [],
      "source": [
        "# Compare two or more models\n",
        "data=mycompetition.compare_models([1, 2, 3], verbose=1)\n",
        "mycompetition.stylize_compare(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 more models [30 Pts]"
      ],
      "metadata": {
        "id": "-nU8VR9uT3OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train three more prediction models to try to predict the SST sentiment\n",
        "dataset well.\n",
        "\n",
        "○ Use Conv1d layers in first model [ 2 Pts]\n",
        "\n",
        "○ Use Transfer learning with Glove Embeddings for 2nd model [10 Pts]\n",
        "\n",
        "○ Third model can be any Transfer learning model of your choice (Transformer architecture required, eg. BERT and related) [10 Pts]"
      ],
      "metadata": {
        "id": "eefOmAIaJYnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, Dropout\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_model_conv1d(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=40))\n",
        "\n",
        "    # Add Conv1D layers\n",
        "    for i in range(hp.Int('num_conv1d_layers', 1, 3)):  # Number of Conv1D layers (1 to 3)\n",
        "        model.add(Conv1D(filters=hp.Int(f'conv1d_{i}_filters', min_value=32, max_value=128, step=32),\n",
        "                         kernel_size=hp.Int(f'conv1d_{i}_kernel_size', min_value=3, max_value=5, step=2),\n",
        "                         activation='relu'))\n",
        "        model.add(GlobalMaxPooling1D())  # Apply global max pooling after each Conv1D layer\n",
        "\n",
        "    # Add Dense layers\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))  # Dropout layer for regularization\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model_conv1d,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10\n",
        ")\n",
        "\n",
        "# Run the tuner\n",
        "tuner.search(x_val, y_val, epochs=30)\n",
        "\n",
        "# Build model with best hyperparameters\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=2)  # Get the top 2 hyperparameters\n",
        "\n",
        "# Iterate over the top hyperparameters\n",
        "for hp in best_hps:\n",
        "    # Build the model with the best hyperparameters\n",
        "    model = build_model_conv1d(hp)\n",
        "    # Fit the model with the entire dataset\n",
        "    model.fit(preprocessor(X_train)[0], y_train, epochs=30, batch_size=32, callbacks = [red_lr, mod_check, early_stop])"
      ],
      "metadata": {
        "id": "xNzUJrsxJlU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, Dropout\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "embeddings_index = {}\n",
        "embedding_dim = 100  # Assuming you want to use 100-dimensional GloVe embeddings\n",
        "glove_file = 'path_to_glove/glove.6B.100d.txt'  # Path to the GloVe embeddings file\n",
        "\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Prepare embedding matrix\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define the model architecture\n",
        "def build_model_glove(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=num_words, output_dim=embedding_dim,\n",
        "                        embeddings_initializer=Constant(embedding_matrix),\n",
        "                        input_length=40, trainable=False))  # Use pre-trained GloVe embeddings\n",
        "\n",
        "    for i in range(hp.Int('num_lstm_layers', 1, 3)):  # Number of LSTM layers (1 to 3)\n",
        "        model.add(LSTM(units=hp.Int(f'lstm_{i}_units', min_value=32, max_value=128, step=32),\n",
        "                       return_sequences=True))\n",
        "        model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)))  # Dropout after each LSTM layer\n",
        "\n",
        "    model.add(LSTM(16))  # Final LSTM layer\n",
        "    model.add(Dropout(0.2))  # Dropout after the final LSTM layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model_glove,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10\n",
        ")\n",
        "\n",
        "# Run the tuner\n",
        "tuner.search(x_val, y_val, epochs=30)\n",
        "\n",
        "# Build model with best hyperparameters\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=2)  # Get the top 2 hyperparameters\n",
        "\n",
        "# Iterate over the top hyperparameters\n",
        "for hp in best_hps:\n",
        "    # Build the model with the best hyperparameters\n",
        "    model = build_model_glove(hp)\n",
        "    # Fit the model with the entire dataset\n",
        "    model.fit(preprocessor(X_train)[0], y_train, epochs=30, batch_size=32, callbacks = [red_lr, mod_check, early_stop])"
      ],
      "metadata": {
        "id": "kYqWJ8VdJ_kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the pre-trained GloVe embeddings and prepare an embedding matrix using the words in the tokenizer's vocabulary.\n",
        "The Embedding layer of the model is initialized with the pre-trained GloVe embeddings using the Constant initializer, and we set trainable=False to freeze the embeddings during training.\n",
        "The rest of the model architecture remains the same, and we use the Keras Tuner to search for the best hyperparameters while keeping the GloVe embeddings fixed."
      ],
      "metadata": {
        "id": "J-VSBiHhW6cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3\n"
      ],
      "metadata": {
        "id": "SmWC5tG0J_nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra credits : Train any models of your choice [+10 Extra Points]"
      ],
      "metadata": {
        "id": "PKjNrHW2fZOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Any modern models of your choice (should be released after BERT)"
      ],
      "metadata": {
        "id": "Mjfe4bPZflxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tabularize results from ALL your models in this notebook and compare performance [5 Pts]"
      ],
      "metadata": {
        "id": "E15c5GrbKT_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to evaluate model performance\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# List of saved models\n",
        "models = []\n",
        "\n",
        "# Evaluate the models on the test data to obtain performance metrics\n",
        "test_metrics = []\n",
        "for model in models:\n",
        "  test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=1) ## X_test, y_test??\n",
        "  test_metrics.append({'Test Loss': test_loss,\n",
        "                       'Test Accuracy': test_accuracy,\n",
        "                       'Test Precision': test_precision,\n",
        "                       'Test Recall': test_recall})"
      ],
      "metadata": {
        "id": "FLbR4oEmLJJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame({\n",
        "    'Model': ['','',''],\n",
        "    'Accuracy': [metrics['Test Accuracy'] for metrics in test_metrics],\n",
        "    'Loss': [metrics['Test Loss'] for metrics in test_metrics],\n",
        "    'Precision': [metrics['Test Precision'] for metrics in test_metrics],\n",
        "    'Recall': [metrics['Test Recall'] for metrics in test_metrics]\n",
        "    })\n",
        "\n",
        "print(metrics_df)"
      ],
      "metadata": {
        "id": "0hT8ALqSXzi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwB8haTTK6Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results Discussion : Point out why specific models may have performed better than others. and discuss failures if any. [3 Pts]"
      ],
      "metadata": {
        "id": "-NH7R7sNLGLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Inference pipeline on unseen dataset [45 Points (35 Pts + 10 Extra Credits)]"
      ],
      "metadata": {
        "id": "UBwhx3wfUIsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Github Repo: </b>\n",
        "\n",
        "## **Part 1. [25 Pts]**\n",
        "\n",
        "You will exercise **Modularization** and Good Github organization: You should take the relevant code and make it into classes across different python (.py) files so that you can just import those files and/or classes and/or functions and/or modules, instantiate the model using them, load weights from your saved models, and start predicting on any new set of test reviews (an unseen larger test dataset).\n",
        "\n",
        "Essentially, you should have a simple and clean **\"front-end\" notebook** where you download your trained and saved models from say a personal drive link, and have an inference pipeline ready within about **10 lines of code** to predict on any new unseen Movie reviews dataset after loading it from a csv.\n",
        "\n",
        "This time you will provide a choice of 3 trained models to choose from (atleast 2 from the last 3 models section + extra credit section if you did it) . You will give us a list of strings as a comment, and should provide the functionality to just specify the model to load using one of the 3 strings. An outline is detailed below.\n",
        "\n",
        "<br><br>\n",
        "**We have provided a test_sample.csv with 100 rows for you to test your front-end notebook.**\n",
        "\n",
        "Your notebook should import relevant classes and modules on the back-end, and should load weights and subsequently just take the \"review\" column of given test_sample.csv and predict 'positive' or 'negative' sentiment on it (see outline of code below). Use the \"sentiment\" column from the test_sample csv ONLY when finally comparing your own predictions with it and output a classification report.\n",
        "\n",
        "You should use a local environment with all packages installed and an IDE like VScode or PyCharm etc. to be able to do this section efficiently. There is no aimodelshare invlved in this section, and there shouldn't be any mention of it.\n",
        "\n",
        "\n",
        "\n",
        "<br><br>\n",
        "### Environment Requirements\n",
        "Include a pip requirements.txt file for your environment.\n",
        "Use the following command to generate it automatically:\n",
        "\n",
        "```$ pip freeze > requirements.txt```\n",
        "\n",
        "When evaluating this section, we will run\n",
        "\n",
        "```pip install -r requirements.txt```\n",
        "\n",
        "to recreate the exact environment you have locally and then test your code.\n",
        "\n",
        "**ENSURE** that this works as intended by creating another new virtual conda environment, cloning your own latest Github commit locally from scratch, installing packages using your requirements.txt file, and following all the instructions you provide in the Readme and the front-end notebook to run the inference pipeline on the sample data. We will do the exact same steps to replicate it, and then substitute test sample with the actual dataset to evaluate this section.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nHZKXkVAPUbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline for front-end notebook:"
      ],
      "metadata": {
        "id": "1DfTz2lgAIqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## You will include code to load the test_sample.csv in your Front-end notebok\n",
        "## Note that this csv can be of any large length, and you will predict on it using one of 3 models you provide weights for."
      ],
      "metadata": {
        "id": "fGSNFMqRPvWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide a comment like this:\n",
        "\n",
        "\n",
        "\n",
        "### Hi User! Please choose a model between [\"Glove150d\", \"DistillBERT\", \"GPT2\"]\n",
        "\n",
        "model_name = \"DistillBERT\"  # Choice to modify model name from your given list\n",
        "\n",
        "\n",
        "## Now call appropriate class/function from backend to download relevant weights, instantiate specified model and load the weights, and return the loaded model, ready to predict\n",
        "\n",
        "## load the test csv:\n",
        "import pandas as pd\n",
        "\n",
        "test_data = pd.read_csv(\"test_sample.csv\")\n",
        "test_reviews=test_data.review\n"
      ],
      "metadata": {
        "id": "bGTkJpjWP4VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Send the test reviews to a backend function after preprocessing to predict sentiment : USE \"tqdm\" while predicting to show progress bar, transform predictions as needed\n",
        "\n",
        "## preprocessing and any other intermediate steps should be through backend code, not in the front-end notebook\n",
        "\n",
        "## pull out test_data.sentiment now (ground truth), transform as needed and compare your predictions with it\n",
        "\n",
        "## Output the classification report on these test reviews and plot confusion matrix at the end of the front-end notebook (You will actually be tested on a much larger test dataset, but it would be of the same format)"
      ],
      "metadata": {
        "id": "HdmOat5TQPPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make sure your notebook can run your inference pipeline on the loaded test_sample.csv reviews.  \n",
        "\n",
        "# **You will recieve a ZERO (0) for this section if any of the following cases are true:**\n",
        "### - If there is no front-end notebook, or no python (.py) files where you have declared classes, modules and functions to delegate all the work in backend\n",
        "### - If there is no requirements.txt file in your repo\n",
        "### - If after installing requirements in a new environment, following the  instructions in your Readme or Notebook does not lead to any valid output upon running the front-end notebook.\n",
        "### - If your notebook does not output the final classification report and matrix for at least 1 model after predicting on test_sample\n",
        "### - If you include or run any Training code in your front-end notebook or python files in github repository\n",
        "### - If you include more than 30 lines of code in the front-end notebook (apart from importing your functions, classes from your modules)\n",
        "### - If you attempt to use the 'sentiment' column from the test_sample for any  purpose other than comparison of your final predictions with it, and transforming it if needed.\n",
        "\n",
        "\n",
        "\n",
        "<br><br>\n",
        "# **You will recieve 25 + 10 EXTRA points if you do this section well.**\n",
        "\n",
        "### How to do this section well:\n",
        "\n",
        "Get a proper understanding of object-oriented programming in python. Learn what classes and modules are if you don't know already. Try to **encapsulate all code** within appropriately named classes and its functions. Each class and function should have a specific isloated purpose. All the classes and functions you design would talk to each other efficiently and handle everything in the background without needing to get back to the front-end notebook for outputting or retreiving intermediate objects.\n",
        "\n",
        "In the front-end notebook, you essentially only need to include code to send the model name to backend using 1 function, get the loaded model back, read and send the test reviews to backend for predicting with this loaded model, then load the ground truth and straight up output the classification report from the backend (less than 10 lines of code)\n",
        "\n",
        "The majority of your code should never be exposed directly in python files without being inside a class or a function. Only explicitly needed global variables, imports or declarations should be done outside the scope of a function or a class.\n",
        "\n",
        "<br><br>\n",
        "### Documentation\n",
        "\n",
        "Include **Docstrings** for each function and class, define the input parameters and outputs, what they are and what data type they are expected to be in.\n",
        "\n",
        "Example of a good docstring:\n",
        "```python\n",
        "def calculate_rectangle_area(length: float, width: float) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the area of a rectangle.\n",
        "\n",
        "    This function takes the length and width of a rectangle and returns its area. The calculation assumes that the input values are in the same unit and will return the area in that unit squared.\n",
        "\n",
        "    Parameters:\n",
        "    - length (float): The length of the rectangle. Must be a positive number.\n",
        "    - width (float): The width of the rectangle. Must be a positive number.\n",
        "\n",
        "    Returns:\n",
        "    - area (float): The calculated area of the rectangle in the unit squared of the input measurements.\n",
        "\n",
        "    Raises:\n",
        "    - ValueError: If either the length or width is non-positive.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if length <= 0 or width <= 0:\n",
        "        raise ValueError(\"Length and width must be positive numbers.\")\n",
        "\n",
        "    area = length * width\n",
        "\n",
        "    return area\n",
        "```"
      ],
      "metadata": {
        "id": "7DMPWpf6bY6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<br><br>\n",
        "### **Part 2. [5 Pts]**\n",
        "\n",
        "In the **README.md** file of your repository should be an overall outline of your experiments, including the models you trained succesfully and have results for - summarized in a tabular way. You can include other details in your readme as well, but not too detailed. Consider this an exposition of your project for someone who does not know anything about this project.\n",
        "\n",
        "\n",
        "Include instructions on how to install requirements, how to run your front-end notebook and describe which 3 models you gave choice to load and predict on.\n",
        "\n",
        "\n",
        "Include a tree structure of your files in the github repository.\n",
        "\n",
        "Example:\n",
        "```\n",
        "$ tree\n",
        ".\n",
        "├── dir1\n",
        "│   ├── file11.ext\n",
        "│   └── file12.ext\n",
        "├── dir2\n",
        "│   ├── file21.ext\n",
        "│   ├── file22.ext\n",
        "│   └── file23.ext\n",
        "├── dir3\n",
        "├── file_in_root.ext\n",
        "└── README.md\n",
        "\n",
        "3 directories, 7 files\n",
        "```\n",
        "\n",
        "Look at different readme's across github to see how they are organized. Refer to [this resource](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to learn how to format the file.\n",
        "\n",
        "- Example of a [Good Readme File](https://github.com/hankcs/HanLP) (We have no affiliations with Author)\n",
        "- Example of a [Bad Readme File](https://github.com/tstran155/AG-News-Topic-Classification-and-Topic-Modeling-using-Natural-Language-Processing)\n",
        "- Example of an [Acceptable/OK Readme File](https://github.com/lonePatient/Bert-Multi-Label-Text-Classification)\n",
        ": This is the minimum level at which you should have your Readme\n",
        "\n",
        "Note that these examples are different from the last assignment.\n",
        "\n",
        "<br><br>\n",
        "### **Part 3. [5 Pts]**\n",
        "\n",
        "In the **Final report** (.ipynb) notebook, put whatever code, visuals and results you think are relevant. It is open ended, you should not just post a copy of this notebook for it. It should be brief, clean and succinct as has been instructed before. No residues from the assignment should be present, it should look like a professional report of experiments conducted.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tjopqFqUs0Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Do not forget to link the final Github repository containing the Front-end running notebook, your code, Readme and Final report at the top of this notebook."
      ],
      "metadata": {
        "id": "H4dPFDuKU-zN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GPcFanPsU_Qf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}